{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from tictactoe import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy \n",
    "from pprint import pprint\n",
    "from collections import deque\n",
    "from time import time \n",
    "from multiprocessing import Pool \n",
    "\n",
    "import random \n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of Tic-Tac-Toe (https://en.wikipedia.org/wiki/Tic-tac-toe_variants)\n",
    "\n",
    "$m,n,k$ game = play on $m$ by $n$ board to try to get $k$ in a row.\n",
    "\n",
    "We have that TicTacToe extends Game, meaning it must have a checkGameOver function. We modify it so that in the initializer it takes in an additional parameter, 'gameOverChecker', a function that takes a board and logger and returns $0$ if nobody won, $1$ if Player one won, or $2$ if Player $2$ won. \n",
    "\n",
    "This allows us to easily implement any variants of the standard 3x3 game that only modify that winning conditions. For example, Misere Tic-tac-toe, or 'inverse' Tic-Tac-Toe, is the game where Player 1 wins if Player 2 gets 3 in a row (we make wrappers for each of the game versions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to support variants of the game where instead of alternating turns, there is some different rule for check whose turn it is. For this, we will add another parameter to TicTacToe constructor, turnChooser. This is passed into the Game superclass. Note that $turnChooser$ is a function that takes in the current player and gives the next player. This lets us support $random$ $turn$ $tic$-$tac$-$toe$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to support larger boards. To do this, we pass in an additional parameter $dimension$. This then creates a board of size $dimension$ by $dimension$. Although this allows us to define boards of arbitrary size, we create $FourByFourTicTacToe$ and $FiveByFiveTicTacToe$. The most logical win condition for an $n$ by $n$ board is $n$ in a row. For this reason we modify the $checkGameOver$ function to take in $n$ - the number of pieces in a row required for a win. Note that other variants are possible - for example, getting a diamond in $4$ by $4$ could also be considered a win. We can then create an $n$ in a row win condition function by using partial functions with $checkGameOver$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to support games in 3 dimensions. We do this by adding yet another paremeter to the initialization - $threeDims$. In order to make it easier to handle 2D vs 3D games, we will always assume the board is 3D - n by n by n, but for 2D we can just get the n by n board by calling board[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like easily define combinations of these variants to create custom games. In order to do this, we define a TicTacToeConfig class. Now, TicTacToe simply takes a TicTacToeConfig. This config has all of the default values so we can set any combinations of the ones we want. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Board\n",
    "\n",
    "Looking at the board in the command line is annoying, and we would like some way of seeing what the algorithm is actually doing in a way that is easier to interpret. We will use PyGame to do this. First, we create a `display` method in TicTacToe.\n",
    "\n",
    "The correct way to do this would be to have some event that is triggered when we make a move to update the display. Due to lazyness, we will just spawn a different thread. This then renders the grid 60 times per second and colors it according to the current board state. \n",
    "\n",
    "http://programarcadegames.com/index.php?lang=en&chapter=array_backed_grids\n",
    "https://www.pygame.org/docs/tut/ChimpLineByLine.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Value Function with a Neural Network\n",
    "\n",
    "First, we define the neural network. We use tanh to bound the result between -1 and 1 (since this is the bound of our value function). We use standard activation functions, testing first ReLu and then Leaky ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netThree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1f57e8e4e469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#     netFour  = ConvNet(dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnetOne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetTwo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetThree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetFour\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netThree' is not defined"
     ]
    }
   ],
   "source": [
    "# Regular Feed forward network with only dense layers \n",
    "class DenseNetRELU(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(DenseNetRELU, self).__init__()\n",
    "        # 9 input features (each of the positions in the board), with a bias\n",
    "        # 1 hidden layer with 9 inputs, 1 output (the value of the state)\n",
    "        numStates = dimension * dimension\n",
    "        self.first = nn.Linear(numStates, numStates, True)\n",
    "        self.hiddenOne = nn.Linear(numStates, numStates, True)\n",
    "        self.hiddenTwo = nn.Linear(numStates, 1, True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.first(x)) \n",
    "        x = F.relu(self.hiddenOne(x)) \n",
    "        x = F.tanh(self.hiddenTwo(x)) \n",
    "        return x\n",
    "    \n",
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)\n",
    "    \n",
    "class DenseNetLeakyRELU(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(DenseNetLeakyRELU, self).__init__()\n",
    "        \n",
    "        numStates = dimension * dimension\n",
    "        self.first = nn.Linear(numStates, numStates, True)\n",
    "        self.hiddenOne = nn.Linear(numStates, numStates, True)\n",
    "        self.hiddenTwo = nn.Linear(numStates, 1, True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.first(x)) \n",
    "        x = F.leaky_relu(self.hiddenOne(x)) \n",
    "        x = F.tanh(self.hiddenTwo(x)) \n",
    "        return x\n",
    "    \n",
    "# Convolutional neural network (for 2D)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, dimension): \n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0 \n",
    "        \n",
    "\n",
    "nets = dict()\n",
    "\n",
    "for dim in range(3, 11): \n",
    "    netOne   = DenseNetRELU(dim)\n",
    "    netTwo   = DenseNetLeakyRELU(dim)\n",
    "#     netThree = ConvNet(dim)\n",
    "#     netFour  = ConvNet(dim)\n",
    "    \n",
    "    nets[dim] = [netOne, netTwo, netThree, netFour]\n",
    "\n",
    "print(nets[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our game tree by defining a node class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    ''' \n",
    "    Node is a single board state in our game tree.\n",
    "    '''\n",
    "    def __init__(self, board): \n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.board = board\n",
    "        self.currTurn = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make a generic function which will take some starting node and a game constructor (which must create an object that extends the Game abstractclass) and fills out the game tree. It does this more efficiently by using a thread pool. Note that this also returns a list of all the nodes in the tree, which allows us to choose a random node much more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGameTree(game, root, num_nodes): \n",
    "    p = Pool(4)\n",
    "    return findAll(game, root, num_nodes)\n",
    "    \n",
    "def findAll(game, startingNode, num_nodes):\n",
    "    '''\n",
    "    findAll performs BFS from the startingNode. It uses a \n",
    "    '''\n",
    "    visited = [0] * num_nodes\n",
    "    index = 0 \n",
    "\n",
    "    # Initialize a queue with the starting node. \n",
    "    unvisited = deque()\n",
    "    unvisited.append(startingNode)\n",
    "    \n",
    "    # Continue until there are no more unvisited nodes.\n",
    "    while len(unvisited) > 0:\n",
    "        if index % 5000 == 0:\n",
    "            print(index, len(unvisited))\n",
    "        # Store the new visited nodes          \n",
    "        currNode = unvisited.popleft()\n",
    "        index += 1\n",
    "        visited[index] = currNode\n",
    "        \n",
    "        # If game is over, do not add the children\n",
    "        game.board = currNode.board\n",
    "        res = game.checkGameOver()\n",
    "        if res != 0: \n",
    "            continue\n",
    "            \n",
    "        # Find all of the children \n",
    "        for action in game.getAllActions():\n",
    "            child = Node(copy.deepcopy(currNode.board))\n",
    "            child.currTurn = game.turnChooser(currNode.currTurn)\n",
    "            pieceToPlay = 1 if currNode.currTurn == 0 else 2\n",
    "            child.board[action[0]][action[1]][action[2]] = pieceToPlay\n",
    "            child.parent = currNode\n",
    "            \n",
    "            unvisited.append(child)\n",
    "            currNode.children.append(child)\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the game tree for all game variants so we have data to train the neural network. Note that for many games the game tree is small enough to store in memory. There are 9 places to place the first piece, then 8 to place the second, etc.., so there are $9! = 362880$ states, many of which are not reachable because someone would win. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardTicTacToe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1444b493b3e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNUM_NODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardTicTacToe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardTicTacToe' is not defined"
     ]
    }
   ],
   "source": [
    "root = Node([[[0,0,0],[0,0,0],[0,0,0]]])\n",
    "NUM_NODES = 1000000\n",
    "game = StandardTicTacToe(None, None, None)\n",
    "\n",
    "vertices = findAll(game, root, NUM_NODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implement minimax so that we can evaluate a state. We define player 2 winning as -1, and player 1 winning as 1. If currTurn is 0, it's player 1 to move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(state):\n",
    "    game.board = state.board \n",
    "    res = game.getAllActions()\n",
    "    res2 = game.checkGameOver()\n",
    "    if res2 == 1:\n",
    "        return 1 \n",
    "    elif res2 == 2: \n",
    "        return -1\n",
    "    elif len(res) == 0: \n",
    "        return 0\n",
    "        \n",
    "    children = state.children\n",
    "    if state.currTurn == 0: \n",
    "        val = -float('inf')\n",
    "    else: \n",
    "        val = float('inf')\n",
    "        \n",
    "    for child in children: \n",
    "        child_val = minimax(child)\n",
    "        if state.currTurn == 0: \n",
    "            val = max(val, child_val)\n",
    "        else: \n",
    "            val = min(val, child_val)\n",
    "\n",
    "    return val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e3c88ff4d425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrainCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "memo = dict() \n",
    "\n",
    "st = time() \n",
    "\n",
    "trainCount = 500000\n",
    "criterion = nn.MSELoss() # Using mean square error \n",
    "optimizer = optim.SGD(nets[3][0].parameters(), lr=0.01) #  create your optimizer\n",
    "\n",
    "def train(net, trainCount):\n",
    "    L = [0] * trainCount\n",
    "    for poo in range(trainCount):\n",
    "        if poo % 2000 == 0:\n",
    "            pass\n",
    "#             print(f\"{poo}/{trainCount} - {poo/trainCount} - {time() - st}s\", end=\"\\r\")\n",
    "        optimizer.zero_grad()\n",
    "        # First, pick some state from the game tree: \n",
    "        node = random.choice(vertices)\n",
    "        # Now give it to the neural net \n",
    "        if node == 0:  \n",
    "            continue\n",
    "        board = torch.FloatTensor(node.board).reshape(-1)\n",
    "        stateValue = net(board)\n",
    "\n",
    "        if node in memo:\n",
    "            res = memo[node]\n",
    "        else:\n",
    "            res = minimax(node)\n",
    "            memo[node] = res \n",
    "\n",
    "        loss = criterion(stateValue, torch.FloatTensor([res]))\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        L[poo] = loss\n",
    "    return L\n",
    "\n",
    "# with Pool(4) as p:\n",
    "#         p.map(train, [100, 100, 100, 100])\n",
    "\n",
    "results = dict() \n",
    "for dim in range(3, 4):\n",
    "    dimNets = nets[dim]\n",
    "    for netIndex in range(len(dimNets)):\n",
    "        r = train(dimNets[netIndex], 1000)\n",
    "        results[(dim, netIndex)] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-31a349dde769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mamount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "L2 = []\n",
    "for i in range(0, len(L), 2500): \n",
    "    if len(L) - i < 2500:\n",
    "        amount = len(L) - i \n",
    "    else: \n",
    "        amount = 2500 \n",
    "    L2.append(sum(L[i:i+2500])/amount)\n",
    "   \n",
    "plt.plot(L2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we pass this network into an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ValueAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e2b2015e414c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnAgent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetValueFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ValueAgent' is not defined"
     ]
    }
   ],
   "source": [
    "def netValueFunction(board): \n",
    "    tBoard = torch.FloatTensor(board).reshape(-1)\n",
    "    print(tBoard) \n",
    "    out = net(tBoard)\n",
    "    for i, v in enumerate(out): \n",
    "        if v == out.max():\n",
    "            return i\n",
    "        \n",
    "    return None\n",
    "            \n",
    "nAgent = ValueAgent(0, netValueFunction, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons of Greedy Policy to Baseline\n",
    "\n",
    "Here, we compare the greedy policy from the learned value function to an agent that plays random policies and a minimax agent (which plays perfectly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardTicTacToe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-df15822da2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardTicTacToe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDumbAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardTicTacToe' is not defined"
     ]
    }
   ],
   "source": [
    "game = StandardTicTacToe(nAgent, DumbAgent(1), Log(2))\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10): \n",
    "    foo = game.play()\n",
    "    if foo == 0: \n",
    "        print(\"Tie\")\n",
    "        results.append(-1)\n",
    "    else: \n",
    "        results.append(foo)\n",
    "    game.reset()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
